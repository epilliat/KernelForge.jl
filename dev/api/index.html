<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · KernelForge.jl</title><meta name="title" content="API Reference · KernelForge.jl"/><meta property="og:title" content="API Reference · KernelForge.jl"/><meta property="twitter:title" content="API Reference · KernelForge.jl"/><meta name="description" content="Documentation for KernelForge.jl."/><meta property="og:description" content="Documentation for KernelForge.jl."/><meta property="twitter:description" content="Documentation for KernelForge.jl."/><meta property="og:url" content="https://epilliat.github.io/KernelForge.jl/stable/api/"/><meta property="twitter:url" content="https://epilliat.github.io/KernelForge.jl/stable/api/"/><link rel="canonical" href="https://epilliat.github.io/KernelForge.jl/stable/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">KernelForge.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../performances/">Performance</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Copy"><span>Copy</span></a></li><li><a class="tocitem" href="#Map-Reduce"><span>Map-Reduce</span></a></li><li><a class="tocitem" href="#Scan"><span>Scan</span></a></li><li><a class="tocitem" href="#Search"><span>Search</span></a></li><li><a class="tocitem" href="#Matrix-Vector"><span>Matrix-Vector</span></a></li><li><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/epilliat/KernelForge.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/epilliat/KernelForge.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><h2 id="Copy"><a class="docs-heading-anchor" href="#Copy">Copy</a><a id="Copy-1"></a><a class="docs-heading-anchor-permalink" href="#Copy" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.vcopy!"><a class="docstring-binding" href="#KernelForge.vcopy!"><code>KernelForge.vcopy!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">vcopy!(dst::AbstractGPUVector, src::AbstractGPUVector; Nitem=4)</code></pre><p>Copy <code>src</code> to <code>dst</code> using vectorized GPU memory access.</p><p>Performs a high-throughput copy by loading and storing <code>Nitem</code> elements per thread, reducing memory transaction overhead compared to scalar copies.</p><p><strong>Arguments</strong></p><ul><li><code>dst</code>: Destination GPU vector</li><li><code>src</code>: Source GPU vector (must have same length as <code>dst</code>)</li><li><code>Nitem=4</code>: Number of elements processed per thread. Higher values improve throughput but require <code>length(src)</code> to be divisible by <code>Nitem</code>.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">src = CUDA.rand(Float32, 1024)
dst = CUDA.zeros(Float32, 1024)
vcopy!(dst, src)</code></pre><p>See also: <a href="#KernelForge.setvalue!"><code>KernelForge.setvalue!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/copy/copy.jl#L1-L23">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.setvalue!"><a class="docstring-binding" href="#KernelForge.setvalue!"><code>KernelForge.setvalue!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">setvalue!(dst::AbstractGPUVector{T}, val::T; Nitem=4) where T</code></pre><p>Fill <code>dst</code> with <code>val</code> using vectorized GPU memory access.</p><p>Performs a high-throughput fill by storing <code>Nitem</code> copies of <code>val</code> per thread, reducing memory transaction overhead compared to scalar writes.</p><p><strong>Arguments</strong></p><ul><li><code>dst</code>: Destination GPU vector</li><li><code>val</code>: Value to fill (must match element type of <code>dst</code>)</li><li><code>Nitem=4</code>: Number of elements written per thread. Higher values improve throughput but require <code>length(dst)</code> to be divisible by <code>Nitem</code>.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">dst = CUDA.zeros(Float32, 1024)
setvalue!(dst, 1.0f0)</code></pre><p>See also: <a href="#KernelForge.vcopy!"><code>KernelForge.vcopy!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/copy/copy.jl#L30-L51">source</a></section></details></article><h2 id="Map-Reduce"><a class="docs-heading-anchor" href="#Map-Reduce">Map-Reduce</a><a id="Map-Reduce-1"></a><a class="docs-heading-anchor-permalink" href="#Map-Reduce" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce"><a class="docstring-binding" href="#KernelForge.mapreduce"><code>KernelForge.mapreduce</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce(f, op, src::AbstractArray; dims=nothing, kwargs...) -&gt; scalar or GPU array</code></pre><p>GPU parallel map-reduce operation with optional dimension reduction.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>dims=nothing</code>: Dimensions to reduce over. Options:<ul><li><code>nothing</code> or <code>:</code>: Reduce over all dimensions → 1-element GPU array</li><li><code>Int</code> or <code>Tuple{Int...}</code>: Reduce over those dims → GPU array</li></ul></li><li><code>g=identity</code>: Post-reduction transformation</li><li>Additional kwargs passed to underlying implementations</li></ul><p><strong>Fast paths</strong></p><ul><li>Full reduction (<code>dims=nothing</code>) → <code>mapreduce1d</code></li><li>All dims explicit → <code>mapreduce1d</code> (returns GPU array)</li><li>Contiguous leading dims <code>(1,...,k)</code> → reshape, <code>mapreduce2d</code> on dim 1, reshape back</li><li>Contiguous trailing dims <code>(k,...,n)</code> → reshape, <code>mapreduce2d</code> on dim 2, reshape back</li><li>Both leading and trailing contiguous blocks → two <code>mapreduce2d</code> passes</li><li>General dims → <code>mapreduce_dims</code> fallback</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 100, 50, 20)

# Full reduction → 1-element GPU array
total = mapreduce(identity, +, A)

# Transfer to CPU as usual
scalar = Array(mapreduce(identity, +, A))[]

# Reduce along dim 1: (100, 50, 20) -&gt; (1, 50, 20)
col_sums = mapreduce(identity, +, A; dims=1)

# Reduce along dims (1,2): (100, 50, 20) -&gt; (1, 1, 20)
plane_sums = mapreduce(identity, +, A; dims=(1,2))

# Reduce along last dim: (100, 50, 20) -&gt; (100, 50, 1)
depth_sums = mapreduce(identity, +, A; dims=3)</code></pre><p>See also: <a href="#KernelForge.mapreduce!"><code>KernelForge.mapreduce!</code></a>, <a href="#KernelForge.mapreduce1d"><code>mapreduce1d</code></a>, <a href="#KernelForge.mapreduce2d"><code>mapreduce2d</code></a>, <a href="#KernelForge.mapreduce_dims"><code>mapreduce_dims</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/mapreduce.jl#L5-L51">source</a></section><section><div><pre><code class="language-julia hljs">mapreduce(f, op, srcs::NTuple; kwargs...)</code></pre><p>Multi-array mapreduce. Only supports full reduction (<code>dims=nothing</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/mapreduce.jl#L204-L208">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce!"><a class="docstring-binding" href="#KernelForge.mapreduce!"><code>KernelForge.mapreduce!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce!(f, op, dst, src; dims=nothing, kwargs...)</code></pre><p>In-place GPU parallel map-reduce with dimension support.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 100, 50)
dst = CUDA.zeros(Float32, 1, 50)
mapreduce!(identity, +, dst, A; dims=1)</code></pre><p>See also: <a href="#KernelForge.mapreduce"><code>KernelForge.mapreduce</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/mapreduce.jl#L123-L136">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce2d"><a class="docstring-binding" href="#KernelForge.mapreduce2d"><code>KernelForge.mapreduce2d</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce2d(f, op, src, dim; kwargs...) -&gt; Vector</code></pre><p>GPU parallel reduction along dimension <code>dim</code>.</p><ul><li><code>dim=1</code>: Column-wise reduction (vertical), output length = number of columns</li><li><code>dim=2</code>: Row-wise reduction (horizontal), output length = number of rows</li></ul><p><strong>Arguments</strong></p><ul><li><code>f</code>: Element-wise transformation</li><li><code>op</code>: Reduction operator</li><li><code>src</code>: Input matrix of size <code>(n, p)</code></li><li><code>dim</code>: Dimension to reduce along (1 or 2)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p>For <code>dim=1</code> (column-wise):</p><ul><li><code>Nitem=nothing</code>: Items per thread</li><li><code>Nthreads=nothing</code>: Threads per column reduction</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks=nothing</code>: Number of blocks</li></ul><p>For <code>dim=2</code> (row-wise):</p><ul><li><code>chunksz=nothing</code>: Chunk size for row processing</li><li><code>Nblocks=nothing</code>: Number of blocks per row</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks_row=nothing</code>: Blocks per row</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)

# Column sums (reduce along dim=1)
col_sums = mapreduce2d(identity, +, A, 1)

# Row maximums (reduce along dim=2)
row_maxs = mapreduce2d(identity, max, A, 2)

# Column means
col_means = mapreduce2d(identity, +, A, 1; g=x -&gt; x / size(A, 1))

# Sum of squares per row
row_ss = mapreduce2d(abs2, +, A, 2)</code></pre><p>See also: <a href="#KernelForge.mapreduce2d!"><code>KernelForge.mapreduce2d!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/2D/mapreduce2d.jl#L1-L50">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce2d!"><a class="docstring-binding" href="#KernelForge.mapreduce2d!"><code>KernelForge.mapreduce2d!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce2d!(f, op, dst, src, dim; kwargs...)</code></pre><p>In-place GPU parallel reduction along dimension <code>dim</code>.</p><ul><li><code>dim=1</code>: Column-wise reduction (vertical), <code>dst</code> length = number of columns</li><li><code>dim=2</code>: Row-wise reduction (horizontal), <code>dst</code> length = number of rows</li></ul><p><strong>Arguments</strong></p><ul><li><code>f</code>: Element-wise transformation</li><li><code>op</code>: Reduction operator</li><li><code>dst</code>: Output vector</li><li><code>src</code>: Input matrix of size <code>(n, p)</code></li><li><code>dim</code>: Dimension to reduce along (1 or 2)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p>For <code>dim=1</code> (column-wise):</p><ul><li><code>Nitem=nothing</code>: Items per thread</li><li><code>Nthreads=nothing</code>: Threads per column reduction</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks=nothing</code>: Number of blocks</li></ul><p>For <code>dim=2</code> (row-wise):</p><ul><li><code>chunksz=nothing</code>: Chunk size for row processing</li><li><code>Nblocks=nothing</code>: Number of blocks per row</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks_row=nothing</code>: Blocks per row</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)
col_sums = CUDA.zeros(Float32, 500)
row_maxs = CUDA.zeros(Float32, 1000)

# Column sums
mapreduce2d!(identity, +, col_sums, A, 1)

# Row maximums
mapreduce2d!(identity, max, row_maxs, A, 2)</code></pre><p>See also: <a href="#KernelForge.mapreduce2d"><code>KernelForge.mapreduce2d</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/2D/mapreduce2d.jl#L69-L115">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce1d"><a class="docstring-binding" href="#KernelForge.mapreduce1d"><code>KernelForge.mapreduce1d</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce1d(f, op, src; kwargs...) -&gt; scalar or GPU array
mapreduce1d(f, op, srcs::NTuple; kwargs...) -&gt; scalar or GPU array</code></pre><p>GPU parallel map-reduce operation.</p><p>Applies <code>f</code> to each element, reduces with <code>op</code>, and optionally applies <code>g</code> to the final result.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>src</code> or <code>srcs</code>: Input GPU array(s)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation applied to final result</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li><li><code>to_cpu=true</code>: If true, return scalar; otherwise return 1-element GPU array</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Sum of squares
x = CUDA.rand(Float32, 10_000)
result = mapreduce1d(x -&gt; x^2, +, x)

# Return GPU array instead of scalar
result = mapreduce1d(x -&gt; x^2, +, x; to_cpu=false)

# Dot product of two arrays
x, y = CUDA.rand(Float32, 10_000), CUDA.rand(Float32, 10_000)
result = mapreduce1d((a, b) -&gt; a * b, +, (x, y))</code></pre><p>See also: <a href="#KernelForge.mapreduce1d!"><code>KernelForge.mapreduce1d!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/1D/mapreduce1d.jl#L1-L38">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce1d!"><a class="docstring-binding" href="#KernelForge.mapreduce1d!"><code>KernelForge.mapreduce1d!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce1d!(f, op, dst, src; kwargs...)
mapreduce1d!(f, op, dst, srcs::NTuple; kwargs...)</code></pre><p>In-place GPU parallel map-reduce, writing result to <code>dst[1]</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>dst</code>: Output array (result written to first element)</li><li><code>src</code> or <code>srcs</code>: Input GPU array(s)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation applied to final result</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
dst = CUDA.zeros(Float32, 1)

# Sum
mapreduce1d!(identity, +, dst, x)

# With pre-allocated temporary for repeated calls
tmp = KernelForge.get_allocation(MapReduce1D, x; out_eltype=Float32)
for i in 1:100
    mapreduce1d!(identity, +, dst, x; tmp)
end</code></pre><p>See also: <a href="#KernelForge.mapreduce1d"><code>KernelForge.mapreduce1d</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/1D/mapreduce1d.jl#L41-L77">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce_dims"><a class="docstring-binding" href="#KernelForge.mapreduce_dims"><code>KernelForge.mapreduce_dims</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce_dims(f, op, src, dims; kwargs...) -&gt; GPU array</code></pre><p>GPU parallel map-reduce over specified dimensions.</p><p>Applies <code>f</code> to each element, reduces along <code>dims</code> with <code>op</code>, and optionally applies <code>g</code> to each final element.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>src</code>: Input GPU array</li><li><code>dims</code>: Dimension(s) to reduce over (Int or tuple of Ints)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation applied to each result element</li><li><code>workgroup=256</code>: Workgroup size</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Sum along rows (reduce dim 1)
x = CUDA.rand(Float32, 128, 64)
result = mapreduce_dims(identity, +, x, 1)   # shape: (1, 64)

# Sum of squares along columns (reduce dim 2)
result = mapreduce_dims(x -&gt; x^2, +, x, 2)  # shape: (128, 1)

# Reduce multiple dimensions
x = CUDA.rand(Float32, 4, 8, 16)
result = mapreduce_dims(identity, +, x, (1, 3))  # shape: (1, 8, 1)</code></pre><p>See also: <a href="#KernelForge.mapreduce_dims!"><code>KernelForge.mapreduce_dims!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/ND/mapreduce_dims.jl#L1-L34">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.mapreduce_dims!"><a class="docstring-binding" href="#KernelForge.mapreduce_dims!"><code>KernelForge.mapreduce_dims!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mapreduce_dims!(f, op, dst, src, dims; kwargs...)</code></pre><p>In-place GPU parallel map-reduce over specified dimensions, writing result to <code>dst</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>op</code>: Associative binary reduction operator</li><li><code>dst</code>: Output array (must have size 1 along each reduced dimension)</li><li><code>src</code>: Input GPU array</li><li><code>dims</code>: Dimension(s) to reduce over (Int or tuple of Ints)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Post-reduction transformation applied to each result element</li><li><code>workgroup=256</code>: Workgroup size</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 128, 64)
dst = CUDA.zeros(Float32, 1, 64)

# Sum along dim 1
mapreduce_dims!(identity, +, dst, x, 1)

# Sum of squares along dim 2 with pre-allocated dst
dst2 = CUDA.zeros(Float32, 128, 1)
mapreduce_dims!(x -&gt; x^2, +, dst2, x, 2)</code></pre><p>See also: <a href="#KernelForge.mapreduce_dims"><code>KernelForge.mapreduce_dims</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/ND/mapreduce_dims.jl#L37-L67">source</a></section></details></article><h2 id="Scan"><a class="docs-heading-anchor" href="#Scan">Scan</a><a id="Scan-1"></a><a class="docs-heading-anchor-permalink" href="#Scan" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.scan"><a class="docstring-binding" href="#KernelForge.scan"><code>KernelForge.scan</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">scan(f, op, src; kwargs...) -&gt; GPU array
scan(op, src; kwargs...) -&gt; GPU array</code></pre><p>GPU parallel prefix scan (cumulative reduction) using a decoupled lookback algorithm.</p><p>Applies <code>f</code> to each element, then computes inclusive prefix scan with <code>op</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element (defaults to <code>identity</code>)</li><li><code>op</code>: Associative binary scan operator</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Cumulative sum
x = CUDA.rand(Float32, 10_000)
result = scan(+, x)

# Cumulative sum of squares
result = scan(x -&gt; x^2, +, x)

# With pre-allocated temporary for repeated calls
tmp = KernelForge.get_allocation(Scan1D, similar(x), x)
result = scan(+, x; tmp)</code></pre><p>See also: <a href="#KernelForge.scan!"><code>KernelForge.scan!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/scan/scan.jl#L1-L35">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.scan!"><a class="docstring-binding" href="#KernelForge.scan!"><code>KernelForge.scan!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">scan!(f, op, dst, src; kwargs...)
scan!(op, dst, src; kwargs...)</code></pre><p>In-place GPU parallel prefix scan using a decoupled lookback algorithm.</p><p>Applies <code>f</code> to each element, then computes inclusive prefix scan with <code>op</code>, writing results to <code>dst</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element (defaults to <code>identity</code>)</li><li><code>op</code>: Associative binary scan operator</li><li><code>dst</code>: Output array for scan results</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>tmp=nothing</code>: Pre-allocated temporary buffer</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
dst = similar(x)

# Cumulative sum
scan!(+, dst, x)

# With pre-allocated temporary for repeated calls
tmp = KernelForge.get_allocation(Scan1D, dst, x)
for i in 1:100
    scan!(+, dst, x; tmp)
end</code></pre><p>See also: <a href="#KernelForge.scan"><code>KernelForge.scan</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/scan/scan.jl#L38-L75">source</a></section></details></article><h2 id="Search"><a class="docs-heading-anchor" href="#Search">Search</a><a id="Search-1"></a><a class="docs-heading-anchor-permalink" href="#Search" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.findfirst"><a class="docstring-binding" href="#KernelForge.findfirst"><code>KernelForge.findfirst</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">findfirst(filtr, src; kwargs...) -&gt; Int or CartesianIndex or nothing</code></pre><p>GPU parallel findfirst. Returns the index of the first element in <code>src</code> for which <code>filtr</code> returns <code>true</code>, or <code>nothing</code> if no such element exists. For multidimensional arrays, returns a <code>CartesianIndex</code>.</p><p><strong>Arguments</strong></p><ul><li><code>filtr</code>: Predicate function</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = adapt(backend, rand(Float32, 10_000))
findfirst(&gt;(0.99f0), x)       # returns a linear index or nothing
A = adapt(backend, rand(Float32, 100, 100))
findfirst(&gt;(0.99f0), A)       # returns a CartesianIndex or nothing</code></pre><p>See also: <a href="#KernelForge.findlast"><code>KernelForge.findlast</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/findfirst.jl#L1-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.findlast"><a class="docstring-binding" href="#KernelForge.findlast"><code>KernelForge.findlast</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">findlast(filtr, src; kwargs...) -&gt; Int or CartesianIndex or nothing</code></pre><p>GPU parallel findlast. Returns the index of the last element in <code>src</code> for which <code>filtr</code> returns <code>true</code>, or <code>nothing</code> if no such element exists. Implemented by reversing <code>src</code> and delegating to <a href="#KernelForge.findfirst"><code>KernelForge.findfirst</code></a>, so it accepts the same keyword arguments. For multidimensional arrays, returns a <code>CartesianIndex</code>.</p><p><strong>Arguments</strong></p><ul><li><code>filtr</code>: Predicate function</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = adapt(backend, rand(Float32, 10_000))
findlast(&gt;(0.99f0), x)        # returns a linear index or nothing
A = adapt(backend, rand(Float32, 100, 100))
findlast(&gt;(0.99f0), A)        # returns a CartesianIndex or nothing</code></pre><p>See also: <a href="#KernelForge.findfirst"><code>KernelForge.findfirst</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/findfirst.jl#L52-L79">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.argmax1d"><a class="docstring-binding" href="#KernelForge.argmax1d"><code>KernelForge.argmax1d</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">argmax1d(f, rel, src; kwargs...) -&gt; Int or GPU array
argmax1d(f, rel, srcs::NTuple; kwargs...) -&gt; Int or GPU array</code></pre><p>GPU parallel argmax/argmin operation.</p><p>Applies <code>f</code> to each element, finds the extremum according to <code>rel</code>, and returns the index of the first extremal element. Ties are broken by smallest index.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>rel</code>: Comparison relation (<code>&gt;</code> for argmax, <code>&lt;</code> for argmin)</li><li><code>src</code> or <code>srcs</code>: Input GPU array(s)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>tmp=nothing</code>: Pre-allocated temporary buffer (see <a href="#KernelForge.get_allocation"><code>get_allocation</code></a>)</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if <code>nothing</code>)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li><li><code>to_cpu=true</code>: If <code>true</code>, return scalar <code>Int</code>; otherwise return 1-element GPU array</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)

# Argmax returning scalar index
idx = argmax1d(identity, &gt;, x)

# Argmax returning 1-element GPU array
idx_gpu = argmax1d(identity, &gt;, x; to_cpu=false)

# Argmin of absolute values
idx = argmax1d(abs, &lt;, x)</code></pre><p>See also: <a href="#KernelForge.argmax1d!"><code>KernelForge.argmax1d!</code></a> for the in-place version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/argmax.jl#L1-L37">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.argmax1d!"><a class="docstring-binding" href="#KernelForge.argmax1d!"><code>KernelForge.argmax1d!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">argmax1d!(f, rel, dst, src; kwargs...)
argmax1d!(f, rel, dst, srcs::NTuple; kwargs...)</code></pre><p>In-place GPU parallel argmax/argmin, writing the index to <code>dst[1]</code>.</p><p>Ties are broken by smallest index.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Map function applied to each element</li><li><code>rel</code>: Comparison relation (<code>&gt;</code> for argmax, <code>&lt;</code> for argmin)</li><li><code>dst</code>: Output array (index written to first element)</li><li><code>src</code> or <code>srcs</code>: Input GPU array(s)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>tmp=nothing</code>: Pre-allocated temporary buffer (see <a href="#KernelForge.get_allocation"><code>get_allocation</code></a>)</li><li><code>Nitem=nothing</code>: Items per thread (auto-selected if <code>nothing</code>)</li><li><code>workgroup=256</code>: Workgroup size</li><li><code>blocks=100</code>: Number of blocks</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
dst = CUDA.zeros(Int, 1)

# Argmax index
argmax1d!(identity, &gt;, dst, x)

# With pre-allocated temporary for repeated calls
tmp = KernelForge.get_allocation(Argmax1D, x)
for i in 1:100
    argmax1d!(identity, &gt;, dst, x; tmp)
end</code></pre><p>See also: <a href="#KernelForge.argmax1d"><code>KernelForge.argmax1d</code></a> for the allocating version.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/argmax.jl#L40-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.argmax"><a class="docstring-binding" href="#KernelForge.argmax"><code>KernelForge.argmax</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">argmax(rel, src::AbstractArray)</code></pre><p>GPU parallel search returning the <code>(value, index)</code> pair of the element that is extremal according to the relation <code>rel</code>. Equivalent to <code>argmax1d(identity, rel, src)</code>.</p><p><strong>Arguments</strong></p><ul><li><code>rel</code>: Comparison relation (e.g. <code>&gt;</code> for maximum, <code>&lt;</code> for minimum)</li><li><code>src</code>: Input GPU array</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CuArray([3f0, 1f0, 4f0, 1f0, 5f0])
argmax(&gt;, x)  # returns (5f0, 5)
argmax(&lt;, x)  # returns (1f0, 2)</code></pre><p>See also: <a href="#KernelForge.argmax1d"><code>KernelForge.argmax1d</code></a>, <a href="#KernelForge.argmin"><code>KernelForge.argmin</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/highlevel.jl#L1-L20">source</a></section><section><div><pre><code class="language-julia hljs">argmax(src::AbstractArray)</code></pre><p>GPU parallel argmax returning the <code>(value, index)</code> pair of the maximum element. Equivalent to <code>argmax(&gt;, src)</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CuArray([3f0, 1f0, 4f0, 1f0, 5f0])
argmax(x)  # returns (5f0, 5)</code></pre><p>See also: <a href="#KernelForge.argmin"><code>KernelForge.argmin</code></a>, <a href="#KernelForge.argmax1d"><code>KernelForge.argmax1d</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/highlevel.jl#L23-L36">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.argmin"><a class="docstring-binding" href="#KernelForge.argmin"><code>KernelForge.argmin</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">argmin(src::AbstractArray)</code></pre><p>GPU parallel argmin returning the <code>(value, index)</code> pair of the minimum element. Equivalent to <code>argmax(&lt;, src)</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CuArray([3f0, 1f0, 4f0, 1f0, 5f0])
argmin(x)  # returns (1f0, 2)</code></pre><p>See also: <a href="#KernelForge.argmax"><code>KernelForge.argmax</code></a>, <a href="#KernelForge.argmax1d"><code>KernelForge.argmax1d</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/highlevel.jl#L39-L52">source</a></section></details></article><h2 id="Matrix-Vector"><a class="docs-heading-anchor" href="#Matrix-Vector">Matrix-Vector</a><a id="Matrix-Vector-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Vector" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.matvec"><a class="docstring-binding" href="#KernelForge.matvec"><code>KernelForge.matvec</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">matvec([f, op,] src::AbstractMatrix, x; kwargs...) -&gt; dst
matvec!([f, op,] dst, src, x; kwargs...)</code></pre><p>Generalized matrix-vector operation with customizable element-wise and reduction operations.</p><p>Computes <code>dst[i] = g(op_j(f(src[i,j], x[j])))</code> for each row <code>i</code>, where <code>op_j</code> denotes reduction over columns. For standard matrix-vector multiplication, this is <code>dst[i] = sum_j(src[i,j] * x[j])</code>.</p><p>The allocating version <code>matvec</code> returns a newly allocated result vector. The in-place version <code>matvec!</code> writes to <code>dst</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Binary operation applied element-wise (default: <code>*</code>)</li><li><code>op</code>: Reduction operation across columns (default: <code>+</code>)</li><li><code>dst</code>: Output vector (in-place versions only)</li><li><code>src</code>: Input matrix</li><li><code>x</code>: Input vector, or <code>nothing</code> for row-wise reduction of <code>src</code> alone</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Unary transformation applied to each reduced row</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer for inter-block communication</li><li><code>chunksz=nothing</code>: Elements per thread (auto-tuned if <code>nothing</code>)</li><li><code>Nblocks=nothing</code>: Number of thread blocks (auto-tuned if <code>nothing</code>)</li><li><code>workgroup=nothing</code>: Threads per block (auto-tuned if <code>nothing</code>)</li><li><code>blocks_row=nothing</code>: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if <code>nothing</code>.</li><li><code>FlagType=UInt8</code>: Integer type for synchronization flags</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)
x = CUDA.rand(Float32, 500)

# Standard matrix-vector multiply: y = A * x
y = matvec(A, x)

# Row-wise sum: y[i] = sum(A[i, :])
y = matvec(A, nothing)

# Row-wise maximum: y[i] = max_j(A[i, j])
y = matvec(identity, max, A, nothing)

# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))
y = matvec((a, b) -&gt; exp(a - b), +, A, x)

# In-place version
dst = CUDA.zeros(Float32, 1000)
matvec!(dst, A, x)</code></pre><p><strong>Extended Help</strong></p><p>For tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from <code>blocks_row</code>. <code>blocks_row</code> is equal to Nblocks for a large row matrix.</p><p>Pre-allocating <code>tmp</code> avoids repeated allocation when calling <code>matvec!</code> in a loop. With <code>FlagType=UInt8</code> (default), the flag buffer must be zeroed before each call. Using <code>FlagType=UInt64</code> skips this zeroing by generating a random target flag at each call; correctness holds with probability <code>1 - n/2^64</code>, which is negligible for practical <code>n</code>. Output element type is inferred as <code>promote_op(g, promote_op(f, eltype(src), eltype(x)))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/2D/matvec.jl#L1-L66">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.matvec!"><a class="docstring-binding" href="#KernelForge.matvec!"><code>KernelForge.matvec!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">matvec([f, op,] src::AbstractMatrix, x; kwargs...) -&gt; dst
matvec!([f, op,] dst, src, x; kwargs...)</code></pre><p>Generalized matrix-vector operation with customizable element-wise and reduction operations.</p><p>Computes <code>dst[i] = g(op_j(f(src[i,j], x[j])))</code> for each row <code>i</code>, where <code>op_j</code> denotes reduction over columns. For standard matrix-vector multiplication, this is <code>dst[i] = sum_j(src[i,j] * x[j])</code>.</p><p>The allocating version <code>matvec</code> returns a newly allocated result vector. The in-place version <code>matvec!</code> writes to <code>dst</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Binary operation applied element-wise (default: <code>*</code>)</li><li><code>op</code>: Reduction operation across columns (default: <code>+</code>)</li><li><code>dst</code>: Output vector (in-place versions only)</li><li><code>src</code>: Input matrix</li><li><code>x</code>: Input vector, or <code>nothing</code> for row-wise reduction of <code>src</code> alone</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Unary transformation applied to each reduced row</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer for inter-block communication</li><li><code>chunksz=nothing</code>: Elements per thread (auto-tuned if <code>nothing</code>)</li><li><code>Nblocks=nothing</code>: Number of thread blocks (auto-tuned if <code>nothing</code>)</li><li><code>workgroup=nothing</code>: Threads per block (auto-tuned if <code>nothing</code>)</li><li><code>blocks_row=nothing</code>: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if <code>nothing</code>.</li><li><code>FlagType=UInt8</code>: Integer type for synchronization flags</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">A = CUDA.rand(Float32, 1000, 500)
x = CUDA.rand(Float32, 500)

# Standard matrix-vector multiply: y = A * x
y = matvec(A, x)

# Row-wise sum: y[i] = sum(A[i, :])
y = matvec(A, nothing)

# Row-wise maximum: y[i] = max_j(A[i, j])
y = matvec(identity, max, A, nothing)

# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))
y = matvec((a, b) -&gt; exp(a - b), +, A, x)

# In-place version
dst = CUDA.zeros(Float32, 1000)
matvec!(dst, A, x)</code></pre><p><strong>Extended Help</strong></p><p>For tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from <code>blocks_row</code>. <code>blocks_row</code> is equal to Nblocks for a large row matrix.</p><p>Pre-allocating <code>tmp</code> avoids repeated allocation when calling <code>matvec!</code> in a loop. With <code>FlagType=UInt8</code> (default), the flag buffer must be zeroed before each call. Using <code>FlagType=UInt64</code> skips this zeroing by generating a random target flag at each call; correctness holds with probability <code>1 - n/2^64</code>, which is negligible for practical <code>n</code>. Output element type is inferred as <code>promote_op(g, promote_op(f, eltype(src), eltype(x)))</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/2D/matvec.jl#L1-L66">source</a></section></details></article><article><details class="docstring" open="true"><summary id="KernelForge.vecmat!"><a class="docstring-binding" href="#KernelForge.vecmat!"><code>KernelForge.vecmat!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">vecmat!(dst, x, A; kwargs...)
vecmat!(f, op, dst, x, A; kwargs...)</code></pre><p>GPU parallel vector-matrix multiplication: <code>dst = g(op(f(x .* A), dims=1))</code>.</p><p>For standard matrix-vector product: <code>vecmat!(dst, x, A)</code> computes <code>dst[j] = sum(x[i] * A[i,j])</code>. When <code>x = nothing</code>, computes column reductions: <code>dst[j] = sum(A[i,j])</code>.</p><p><strong>Arguments</strong></p><ul><li><code>f=identity</code>: Element-wise transformation applied to <code>x[i] * A[i,j]</code> (or <code>A[i,j]</code> if <code>x=nothing</code>)</li><li><code>op=+</code>: Reduction operator</li><li><code>dst</code>: Output vector of length <code>p</code> (number of columns)</li><li><code>x</code>: Input vector of length <code>n</code> (number of rows), or <code>nothing</code> for pure column reduction</li><li><code>A</code>: Input matrix of size <code>(n, p)</code></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>g=identity</code>: Optional post-reduction transformation</li><li><code>tmp=nothing</code>: Pre-allocated temporary buffer (from <code>get_allocation</code>)</li><li><code>Nitem=nothing</code>: Number of items per thread (auto-selected if nothing)</li><li><code>Nthreads=nothing</code>: Number of threads per column reduction</li><li><code>workgroup=nothing</code>: Workgroup size</li><li><code>blocks=nothing</code>: Maximum number of blocks</li><li><code>FlagType=UInt8</code>: Type for synchronization flags</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/2D/vecmat.jl#L1-L25">source</a></section></details></article><h2 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="KernelForge.get_allocation"><a class="docstring-binding" href="#KernelForge.get_allocation"><code>KernelForge.get_allocation</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">get_allocation(::Type{MapReduce1D}, src; blocks=DEFAULT_BLOCKS, out_eltype, FlagType=UInt8)
get_allocation(::Type{MapReduce1D}, srcs::NTuple; blocks=DEFAULT_BLOCKS, out_eltype, FlagType=UInt8)</code></pre><p>Allocate temporary buffer for <code>mapreduce1d!</code>. Useful for repeated reductions.</p><p><strong>Arguments</strong></p><ul><li><code>src</code> or <code>srcs</code>: Input GPU array(s) (used for backend)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>blocks=100</code>: Number of blocks (must match the <code>blocks</code> used in <code>mapreduce1d!</code>)</li><li><code>out_eltype</code>: Element type for intermediate values. Pass <code>promote_op(f, T, ...)</code> for correct inference.</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
tmp = KernelForge.get_allocation(MapReduce1D, x; out_eltype=Float32)
dst = CUDA.zeros(Float32, 1)

for i in 1:100
    mapreduce1d!(identity, +, dst, x; tmp)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/mapreduce/1D/mapreduce1d.jl#L84-L108">source</a></section><section><div><pre><code class="language-julia hljs">get_allocation(::Type{Scan1D}, dst, src; kwargs...)</code></pre><p>Allocate temporary buffer for <code>scan!</code>. Useful for repeated scans.</p><p><strong>Arguments</strong></p><ul><li><code>dst</code>: Output GPU array (used for element type of intermediates)</li><li><code>src</code>: Input GPU array (used for backend)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>Nitem=nothing</code>: Items per thread (auto-selected if nothing)</li><li><code>workgroup=256</code>: Workgroup size (must match the <code>workgroup</code> used in <code>scan!</code>)</li><li><code>FlagType=UInt8</code>: Synchronization flag type</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
dst = similar(x)
tmp = KernelForge.get_allocation(Scan1D, dst, x)

for i in 1:100
    scan!(+, dst, x; tmp)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/scan/scan.jl#L92-L116">source</a></section><section><div><pre><code class="language-julia hljs">get_allocation(::Type{Argmax1D}, src; blocks=100, out_eltype=nothing)</code></pre><p>Allocate temporary buffer for <code>argmax1d!</code>. Useful for repeated reductions.</p><p>The intermediate type is <code>Tuple{out_eltype, Int}</code> to track both value and index.</p><p><strong>Arguments</strong></p><ul><li><code>src</code> or <code>srcs</code>: Input GPU array(s) (used for backend and default element type)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>blocks=100</code>: Number of blocks (must match the <code>blocks</code> used in <code>argmax1d!</code>)</li><li><code>out_eltype=nothing</code>: Element type for intermediate values. If <code>nothing</code>, defaults to the element type of <code>src</code>. For proper type inference, pass <code>promote_op(f, T, ...)</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = CUDA.rand(Float32, 10_000)
tmp = KernelForge.get_allocation(Argmax1D, x)
dst = CUDA.zeros(Int, 1)

for i in 1:100
    argmax1d!(identity, &gt;, dst, x; tmp)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/epilliat/KernelForge.jl/blob/29dcb219166af1446bb1e7218c17c38c442f5076/src/search/argmax.jl#L83-L108">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.17.0 on <span class="colophon-date" title="Sunday 22 February 2026 14:23">Sunday 22 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
