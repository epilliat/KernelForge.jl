var documenterSearchIndex = {"docs":
[{"location":"performances/#Performance","page":"Performance","title":"Performance","text":"KernelForge.jl achieves performance comparable to optimized CUDA C++ libraries such as CUB. Benchmarks report two metrics:\n\nKernel time: Execution time of the main kernel, measured using @profile from CUDA.jl\nOverhead: Total time minus kernel time, including memory allocations and data transfers","category":"section"},{"location":"performances/#Copy","page":"Performance","title":"Copy","text":"CUDA.jl leverages the proprietary libcuda library for memory copies, which internally vectorizes loads and stores. In contrast, the cross-platform GPUArrayCore.jl relies on KernelAbstractions.jl, which does not currently perform vectorization. KernelForge's vcopy! bridges this gap by using vload and vstore operations built on unsafe pointer access via LLVMPtrs from KernelIntrinsics.jl.\n\nThe graph below compares memory bandwidth for Float32 and UInt8 data types. With vectorized loads and stores, KernelForge achieves bandwidth comparable to CUDA.jl for both types. The slight underperformance below the L2 cache threshold stems from our current vectorization factor (×8 for Float32); increasing this to ×16 would close the remaining gap.\n\n(Image: Copy Bandwidth)","category":"section"},{"location":"performances/#Map-Reduce","page":"Performance","title":"Map-Reduce","text":"KernelForge.jl matches CUDA.jl performance on Float32 and significantly outperforms it on smaller types (UInt8, UnitFloat8), even when converting to Float32 during reduction. These gains result from optimized memory access patterns and vectorized loads/stores.\n\n(Image: Map-Reduce Benchmark)","category":"section"},{"location":"performances/#Scan","page":"Performance","title":"Scan","text":"KernelForge's scan kernel rivals CUB performance on Float32 and Float64, while additionally supporting non-commutative operations and custom types such as Quaternions. This is achieved through an efficient decoupled lookback algorithm combined with optimized memory access.\n\n(Image: Scan Benchmark)","category":"section"},{"location":"performances/#Matrix-Vector-Operations","page":"Performance","title":"Matrix-Vector Operations","text":"KernelForge implements matrix-vector and vector-matrix operations for general types and operators. For benchmarking, we compare against CUDA.jl on Float32, which internally calls cuBLAS's gemv routine.\n\nDue to column-major memory layout, matrix-vector and vector-matrix multiplications have fundamentally different access patterns. KernelForge therefore provides separate optimized kernels for each operation.\n\nFor both benchmarks, we fix the total matrix size (n × p) and vary n from 10 to (n × p) / 10, sweeping from tall-narrow to short-wide matrices. The black line indicates the reduced overhead achieved when the user provides pre-allocated temporary memory.\n\nMatrix-Vector (Image: Matrix-Vector Benchmark)\n\nVector-Matrix\n\n(Image: Vector-Matrix Benchmark)","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Copy","page":"API Reference","title":"Copy","text":"","category":"section"},{"location":"api/#Map-Reduce","page":"API Reference","title":"Map-Reduce","text":"","category":"section"},{"location":"api/#Scan","page":"API Reference","title":"Scan","text":"","category":"section"},{"location":"api/#Search","page":"API Reference","title":"Search","text":"","category":"section"},{"location":"api/#Matrix-Vector","page":"API Reference","title":"Matrix-Vector","text":"","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"","category":"section"},{"location":"api/#KernelForge.vcopy!","page":"API Reference","title":"KernelForge.vcopy!","text":"vcopy!(dst::AbstractGPUVector, src::AbstractGPUVector; Nitem=4)\n\nCopy src to dst using vectorized GPU memory access.\n\nPerforms a high-throughput copy by loading and storing Nitem elements per thread, reducing memory transaction overhead compared to scalar copies.\n\nArguments\n\ndst: Destination GPU vector\nsrc: Source GPU vector (must have same length as dst)\nNitem=4: Number of elements processed per thread. Higher values improve throughput but require length(src) to be divisible by Nitem.\n\nExample\n\nsrc = CUDA.rand(Float32, 1024)\ndst = CUDA.zeros(Float32, 1024)\nvcopy!(dst, src)\n\nSee also: KernelForge.setvalue!\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.setvalue!","page":"API Reference","title":"KernelForge.setvalue!","text":"setvalue!(dst::AbstractGPUVector{T}, val::T; Nitem=4) where T\n\nFill dst with val using vectorized GPU memory access.\n\nPerforms a high-throughput fill by storing Nitem copies of val per thread, reducing memory transaction overhead compared to scalar writes.\n\nArguments\n\ndst: Destination GPU vector\nval: Value to fill (must match element type of dst)\nNitem=4: Number of elements written per thread. Higher values improve throughput but require length(dst) to be divisible by Nitem.\n\nExample\n\ndst = CUDA.zeros(Float32, 1024)\nsetvalue!(dst, 1.0f0)\n\nSee also: KernelForge.vcopy!\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce","page":"API Reference","title":"KernelForge.mapreduce","text":"mapreduce(f, op, src::AbstractArray; dims=nothing, kwargs...) -> scalar or GPU array\n\nGPU parallel map-reduce operation with optional dimension reduction.\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\nsrc: Input GPU array\n\nKeyword Arguments\n\ndims=nothing: Dimensions to reduce over. Options:\nnothing or :: Reduce over all dimensions → 1-element GPU array\nInt or Tuple{Int...}: Reduce over those dims → GPU array\ng=identity: Post-reduction transformation\nAdditional kwargs passed to underlying implementations\n\nFast paths\n\nFull reduction (dims=nothing) → mapreduce1d\nAll dims explicit → mapreduce1d (returns GPU array)\nContiguous leading dims (1,...,k) → reshape, mapreduce2d on dim 1, reshape back\nContiguous trailing dims (k,...,n) → reshape, mapreduce2d on dim 2, reshape back\nBoth leading and trailing contiguous blocks → two mapreduce2d passes\nGeneral dims → mapreduce_dims fallback\n\nExamples\n\nA = CUDA.rand(Float32, 100, 50, 20)\n\n# Full reduction → 1-element GPU array\ntotal = mapreduce(identity, +, A)\n\n# Transfer to CPU as usual\nscalar = Array(mapreduce(identity, +, A))[]\n\n# Reduce along dim 1: (100, 50, 20) -> (1, 50, 20)\ncol_sums = mapreduce(identity, +, A; dims=1)\n\n# Reduce along dims (1,2): (100, 50, 20) -> (1, 1, 20)\nplane_sums = mapreduce(identity, +, A; dims=(1,2))\n\n# Reduce along last dim: (100, 50, 20) -> (100, 50, 1)\ndepth_sums = mapreduce(identity, +, A; dims=3)\n\nSee also: KernelForge.mapreduce!, mapreduce1d, mapreduce2d, mapreduce_dims\n\n\n\n\n\nmapreduce(f, op, srcs::NTuple; kwargs...)\n\nMulti-array mapreduce. Only supports full reduction (dims=nothing).\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce!","page":"API Reference","title":"KernelForge.mapreduce!","text":"mapreduce!(f, op, dst, src; dims=nothing, kwargs...)\n\nIn-place GPU parallel map-reduce with dimension support.\n\nExamples\n\nA = CUDA.rand(Float32, 100, 50)\ndst = CUDA.zeros(Float32, 1, 50)\nmapreduce!(identity, +, dst, A; dims=1)\n\nSee also: KernelForge.mapreduce\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce2d","page":"API Reference","title":"KernelForge.mapreduce2d","text":"mapreduce2d(f, op, src, dim; kwargs...) -> Vector\n\nGPU parallel reduction along dimension dim.\n\ndim=1: Column-wise reduction (vertical), output length = number of columns\ndim=2: Row-wise reduction (horizontal), output length = number of rows\n\nArguments\n\nf: Element-wise transformation\nop: Reduction operator\nsrc: Input matrix of size (n, p)\ndim: Dimension to reduce along (1 or 2)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation\ntmp=nothing: Pre-allocated temporary buffer\nFlagType=UInt8: Synchronization flag type\n\nFor dim=1 (column-wise):\n\nNitem=nothing: Items per thread\nNthreads=nothing: Threads per column reduction\nworkgroup=nothing: Workgroup size\nblocks=nothing: Number of blocks\n\nFor dim=2 (row-wise):\n\nchunksz=nothing: Chunk size for row processing\nNblocks=nothing: Number of blocks per row\nworkgroup=nothing: Workgroup size\nblocks_row=nothing: Blocks per row\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Column sums (reduce along dim=1)\ncol_sums = mapreduce2d(identity, +, A, 1)\n\n# Row maximums (reduce along dim=2)\nrow_maxs = mapreduce2d(identity, max, A, 2)\n\n# Column means\ncol_means = mapreduce2d(identity, +, A, 1; g=x -> x / size(A, 1))\n\n# Sum of squares per row\nrow_ss = mapreduce2d(abs2, +, A, 2)\n\nSee also: KernelForge.mapreduce2d! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce2d!","page":"API Reference","title":"KernelForge.mapreduce2d!","text":"mapreduce2d!(f, op, dst, src, dim; kwargs...)\n\nIn-place GPU parallel reduction along dimension dim.\n\ndim=1: Column-wise reduction (vertical), dst length = number of columns\ndim=2: Row-wise reduction (horizontal), dst length = number of rows\n\nArguments\n\nf: Element-wise transformation\nop: Reduction operator\ndst: Output vector\nsrc: Input matrix of size (n, p)\ndim: Dimension to reduce along (1 or 2)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation\ntmp=nothing: Pre-allocated temporary buffer\nFlagType=UInt8: Synchronization flag type\n\nFor dim=1 (column-wise):\n\nNitem=nothing: Items per thread\nNthreads=nothing: Threads per column reduction\nworkgroup=nothing: Workgroup size\nblocks=nothing: Number of blocks\n\nFor dim=2 (row-wise):\n\nchunksz=nothing: Chunk size for row processing\nNblocks=nothing: Number of blocks per row\nworkgroup=nothing: Workgroup size\nblocks_row=nothing: Blocks per row\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\ncol_sums = CUDA.zeros(Float32, 500)\nrow_maxs = CUDA.zeros(Float32, 1000)\n\n# Column sums\nmapreduce2d!(identity, +, col_sums, A, 1)\n\n# Row maximums\nmapreduce2d!(identity, max, row_maxs, A, 2)\n\nSee also: KernelForge.mapreduce2d for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce1d","page":"API Reference","title":"KernelForge.mapreduce1d","text":"mapreduce1d(f, op, src; kwargs...) -> scalar or GPU array\nmapreduce1d(f, op, srcs::NTuple; kwargs...) -> scalar or GPU array\n\nGPU parallel map-reduce operation.\n\nApplies f to each element, reduces with op, and optionally applies g to the final result.\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\nsrc or srcs: Input GPU array(s)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation applied to final result\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\nFlagType=UInt8: Synchronization flag type\nto_cpu=true: If true, return scalar; otherwise return 1-element GPU array\n\nExamples\n\n# Sum of squares\nx = CUDA.rand(Float32, 10_000)\nresult = mapreduce1d(x -> x^2, +, x)\n\n# Return GPU array instead of scalar\nresult = mapreduce1d(x -> x^2, +, x; to_cpu=false)\n\n# Dot product of two arrays\nx, y = CUDA.rand(Float32, 10_000), CUDA.rand(Float32, 10_000)\nresult = mapreduce1d((a, b) -> a * b, +, (x, y))\n\nSee also: KernelForge.mapreduce1d! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce1d!","page":"API Reference","title":"KernelForge.mapreduce1d!","text":"mapreduce1d!(f, op, dst, src; kwargs...)\nmapreduce1d!(f, op, dst, srcs::NTuple; kwargs...)\n\nIn-place GPU parallel map-reduce, writing result to dst[1].\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\ndst: Output array (result written to first element)\nsrc or srcs: Input GPU array(s)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation applied to final result\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ndst = CUDA.zeros(Float32, 1)\n\n# Sum\nmapreduce1d!(identity, +, dst, x)\n\n# With pre-allocated temporary for repeated calls\ntmp = KernelForge.get_allocation(MapReduce1D, x; out_eltype=Float32)\nfor i in 1:100\n    mapreduce1d!(identity, +, dst, x; tmp)\nend\n\nSee also: KernelForge.mapreduce1d for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce_dims","page":"API Reference","title":"KernelForge.mapreduce_dims","text":"mapreduce_dims(f, op, src, dims; kwargs...) -> GPU array\n\nGPU parallel map-reduce over specified dimensions.\n\nApplies f to each element, reduces along dims with op, and optionally applies g to each final element.\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\nsrc: Input GPU array\ndims: Dimension(s) to reduce over (Int or tuple of Ints)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation applied to each result element\nworkgroup=256: Workgroup size\n\nExamples\n\n# Sum along rows (reduce dim 1)\nx = CUDA.rand(Float32, 128, 64)\nresult = mapreduce_dims(identity, +, x, 1)   # shape: (1, 64)\n\n# Sum of squares along columns (reduce dim 2)\nresult = mapreduce_dims(x -> x^2, +, x, 2)  # shape: (128, 1)\n\n# Reduce multiple dimensions\nx = CUDA.rand(Float32, 4, 8, 16)\nresult = mapreduce_dims(identity, +, x, (1, 3))  # shape: (1, 8, 1)\n\nSee also: KernelForge.mapreduce_dims! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.mapreduce_dims!","page":"API Reference","title":"KernelForge.mapreduce_dims!","text":"mapreduce_dims!(f, op, dst, src, dims; kwargs...)\n\nIn-place GPU parallel map-reduce over specified dimensions, writing result to dst.\n\nArguments\n\nf: Map function applied to each element\nop: Associative binary reduction operator\ndst: Output array (must have size 1 along each reduced dimension)\nsrc: Input GPU array\ndims: Dimension(s) to reduce over (Int or tuple of Ints)\n\nKeyword Arguments\n\ng=identity: Post-reduction transformation applied to each result element\nworkgroup=256: Workgroup size\n\nExamples\n\nx = CUDA.rand(Float32, 128, 64)\ndst = CUDA.zeros(Float32, 1, 64)\n\n# Sum along dim 1\nmapreduce_dims!(identity, +, dst, x, 1)\n\n# Sum of squares along dim 2 with pre-allocated dst\ndst2 = CUDA.zeros(Float32, 128, 1)\nmapreduce_dims!(x -> x^2, +, dst2, x, 2)\n\nSee also: KernelForge.mapreduce_dims for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.scan","page":"API Reference","title":"KernelForge.scan","text":"scan(f, op, src; kwargs...) -> GPU array\nscan(op, src; kwargs...) -> GPU array\n\nGPU parallel prefix scan (cumulative reduction) using a decoupled lookback algorithm.\n\nApplies f to each element, then computes inclusive prefix scan with op.\n\nArguments\n\nf: Map function applied to each element (defaults to identity)\nop: Associative binary scan operator\nsrc: Input GPU array\n\nKeyword Arguments\n\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\n# Cumulative sum\nx = CUDA.rand(Float32, 10_000)\nresult = scan(+, x)\n\n# Cumulative sum of squares\nresult = scan(x -> x^2, +, x)\n\n# With pre-allocated temporary for repeated calls\ntmp = KernelForge.get_allocation(Scan1D, similar(x), x)\nresult = scan(+, x; tmp)\n\nSee also: KernelForge.scan! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.scan!","page":"API Reference","title":"KernelForge.scan!","text":"scan!(f, op, dst, src; kwargs...)\nscan!(op, dst, src; kwargs...)\n\nIn-place GPU parallel prefix scan using a decoupled lookback algorithm.\n\nApplies f to each element, then computes inclusive prefix scan with op, writing results to dst.\n\nArguments\n\nf: Map function applied to each element (defaults to identity)\nop: Associative binary scan operator\ndst: Output array for scan results\nsrc: Input GPU array\n\nKeyword Arguments\n\ntmp=nothing: Pre-allocated temporary buffer\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ndst = similar(x)\n\n# Cumulative sum\nscan!(+, dst, x)\n\n# With pre-allocated temporary for repeated calls\ntmp = KernelForge.get_allocation(Scan1D, dst, x)\nfor i in 1:100\n    scan!(+, dst, x; tmp)\nend\n\nSee also: KernelForge.scan for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.findfirst","page":"API Reference","title":"KernelForge.findfirst","text":"findfirst(filtr, src; kwargs...) -> Int or CartesianIndex or nothing\n\nGPU parallel findfirst. Returns the index of the first element in src for which filtr returns true, or nothing if no such element exists. For multidimensional arrays, returns a CartesianIndex.\n\nArguments\n\nfiltr: Predicate function\nsrc: Input GPU array\n\nKeyword Arguments\n\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\n\nExamples\n\nx = adapt(backend, rand(Float32, 10_000))\nfindfirst(>(0.99f0), x)       # returns a linear index or nothing\nA = adapt(backend, rand(Float32, 100, 100))\nfindfirst(>(0.99f0), A)       # returns a CartesianIndex or nothing\n\nSee also: KernelForge.findlast.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.findlast","page":"API Reference","title":"KernelForge.findlast","text":"findlast(filtr, src; kwargs...) -> Int or CartesianIndex or nothing\n\nGPU parallel findlast. Returns the index of the last element in src for which filtr returns true, or nothing if no such element exists. Implemented by reversing src and delegating to KernelForge.findfirst, so it accepts the same keyword arguments. For multidimensional arrays, returns a CartesianIndex.\n\nArguments\n\nfiltr: Predicate function\nsrc: Input GPU array\n\nKeyword Arguments\n\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\n\nExamples\n\nx = adapt(backend, rand(Float32, 10_000))\nfindlast(>(0.99f0), x)        # returns a linear index or nothing\nA = adapt(backend, rand(Float32, 100, 100))\nfindlast(>(0.99f0), A)        # returns a CartesianIndex or nothing\n\nSee also: KernelForge.findfirst.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.argmax1d","page":"API Reference","title":"KernelForge.argmax1d","text":"argmax1d(f, rel, src; kwargs...) -> Int or GPU array\nargmax1d(f, rel, srcs::NTuple; kwargs...) -> Int or GPU array\n\nGPU parallel argmax/argmin operation.\n\nApplies f to each element, finds the extremum according to rel, and returns the index of the first extremal element. Ties are broken by smallest index.\n\nArguments\n\nf: Map function applied to each element\nrel: Comparison relation (> for argmax, < for argmin)\nsrc or srcs: Input GPU array(s)\n\nKeyword Arguments\n\ntmp=nothing: Pre-allocated temporary buffer (see get_allocation)\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\nto_cpu=true: If true, return scalar Int; otherwise return 1-element GPU array\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\n\n# Argmax returning scalar index\nidx = argmax1d(identity, >, x)\n\n# Argmax returning 1-element GPU array\nidx_gpu = argmax1d(identity, >, x; to_cpu=false)\n\n# Argmin of absolute values\nidx = argmax1d(abs, <, x)\n\nSee also: KernelForge.argmax1d! for the in-place version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.argmax1d!","page":"API Reference","title":"KernelForge.argmax1d!","text":"argmax1d!(f, rel, dst, src; kwargs...)\nargmax1d!(f, rel, dst, srcs::NTuple; kwargs...)\n\nIn-place GPU parallel argmax/argmin, writing the index to dst[1].\n\nTies are broken by smallest index.\n\nArguments\n\nf: Map function applied to each element\nrel: Comparison relation (> for argmax, < for argmin)\ndst: Output array (index written to first element)\nsrc or srcs: Input GPU array(s)\n\nKeyword Arguments\n\ntmp=nothing: Pre-allocated temporary buffer (see get_allocation)\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size\nblocks=100: Number of blocks\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ndst = CUDA.zeros(Int, 1)\n\n# Argmax index\nargmax1d!(identity, >, dst, x)\n\n# With pre-allocated temporary for repeated calls\ntmp = KernelForge.get_allocation(Argmax1D, x)\nfor i in 1:100\n    argmax1d!(identity, >, dst, x; tmp)\nend\n\nSee also: KernelForge.argmax1d for the allocating version.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.argmax","page":"API Reference","title":"KernelForge.argmax","text":"argmax(rel, src::AbstractArray)\n\nGPU parallel search returning the (value, index) pair of the element that is extremal according to the relation rel. Equivalent to argmax1d(identity, rel, src).\n\nArguments\n\nrel: Comparison relation (e.g. > for maximum, < for minimum)\nsrc: Input GPU array\n\nExamples\n\nx = CuArray([3f0, 1f0, 4f0, 1f0, 5f0])\nargmax(>, x)  # returns (5f0, 5)\nargmax(<, x)  # returns (1f0, 2)\n\nSee also: KernelForge.argmax1d, KernelForge.argmin.\n\n\n\n\n\nargmax(src::AbstractArray)\n\nGPU parallel argmax returning the (value, index) pair of the maximum element. Equivalent to argmax(>, src).\n\nExamples\n\nx = CuArray([3f0, 1f0, 4f0, 1f0, 5f0])\nargmax(x)  # returns (5f0, 5)\n\nSee also: KernelForge.argmin, KernelForge.argmax1d.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.argmin","page":"API Reference","title":"KernelForge.argmin","text":"argmin(src::AbstractArray)\n\nGPU parallel argmin returning the (value, index) pair of the minimum element. Equivalent to argmax(<, src).\n\nExamples\n\nx = CuArray([3f0, 1f0, 4f0, 1f0, 5f0])\nargmin(x)  # returns (1f0, 2)\n\nSee also: KernelForge.argmax, KernelForge.argmax1d.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.matvec","page":"API Reference","title":"KernelForge.matvec","text":"matvec([f, op,] src::AbstractMatrix, x; kwargs...) -> dst\nmatvec!([f, op,] dst, src, x; kwargs...)\n\nGeneralized matrix-vector operation with customizable element-wise and reduction operations.\n\nComputes dst[i] = g(op_j(f(src[i,j], x[j]))) for each row i, where op_j denotes reduction over columns. For standard matrix-vector multiplication, this is dst[i] = sum_j(src[i,j] * x[j]).\n\nThe allocating version matvec returns a newly allocated result vector. The in-place version matvec! writes to dst.\n\nArguments\n\nf: Binary operation applied element-wise (default: *)\nop: Reduction operation across columns (default: +)\ndst: Output vector (in-place versions only)\nsrc: Input matrix\nx: Input vector, or nothing for row-wise reduction of src alone\n\nKeyword Arguments\n\ng=identity: Unary transformation applied to each reduced row\ntmp=nothing: Pre-allocated temporary buffer for inter-block communication\nchunksz=nothing: Elements per thread (auto-tuned if nothing)\nNblocks=nothing: Number of thread blocks (auto-tuned if nothing)\nworkgroup=nothing: Threads per block (auto-tuned if nothing)\nblocks_row=nothing: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if nothing.\nFlagType=UInt8: Integer type for synchronization flags\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# Standard matrix-vector multiply: y = A * x\ny = matvec(A, x)\n\n# Row-wise sum: y[i] = sum(A[i, :])\ny = matvec(A, nothing)\n\n# Row-wise maximum: y[i] = max_j(A[i, j])\ny = matvec(identity, max, A, nothing)\n\n# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))\ny = matvec((a, b) -> exp(a - b), +, A, x)\n\n# In-place version\ndst = CUDA.zeros(Float32, 1000)\nmatvec!(dst, A, x)\n\nExtended Help\n\nFor tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from blocks_row. blocks_row is equal to Nblocks for a large row matrix.\n\nPre-allocating tmp avoids repeated allocation when calling matvec! in a loop. With FlagType=UInt8 (default), the flag buffer must be zeroed before each call. Using FlagType=UInt64 skips this zeroing by generating a random target flag at each call; correctness holds with probability 1 - n/2^64, which is negligible for practical n. Output element type is inferred as promote_op(g, promote_op(f, eltype(src), eltype(x))).\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.matvec!","page":"API Reference","title":"KernelForge.matvec!","text":"matvec([f, op,] src::AbstractMatrix, x; kwargs...) -> dst\nmatvec!([f, op,] dst, src, x; kwargs...)\n\nGeneralized matrix-vector operation with customizable element-wise and reduction operations.\n\nComputes dst[i] = g(op_j(f(src[i,j], x[j]))) for each row i, where op_j denotes reduction over columns. For standard matrix-vector multiplication, this is dst[i] = sum_j(src[i,j] * x[j]).\n\nThe allocating version matvec returns a newly allocated result vector. The in-place version matvec! writes to dst.\n\nArguments\n\nf: Binary operation applied element-wise (default: *)\nop: Reduction operation across columns (default: +)\ndst: Output vector (in-place versions only)\nsrc: Input matrix\nx: Input vector, or nothing for row-wise reduction of src alone\n\nKeyword Arguments\n\ng=identity: Unary transformation applied to each reduced row\ntmp=nothing: Pre-allocated temporary buffer for inter-block communication\nchunksz=nothing: Elements per thread (auto-tuned if nothing)\nNblocks=nothing: Number of thread blocks (auto-tuned if nothing)\nworkgroup=nothing: Threads per block (auto-tuned if nothing)\nblocks_row=nothing: Number of blocks used to process a single row; relevant only for wide matrices (many columns, few rows) where parallelizing across columns is beneficial. Auto-tuned if nothing.\nFlagType=UInt8: Integer type for synchronization flags\n\nExamples\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# Standard matrix-vector multiply: y = A * x\ny = matvec(A, x)\n\n# Row-wise sum: y[i] = sum(A[i, :])\ny = matvec(A, nothing)\n\n# Row-wise maximum: y[i] = max_j(A[i, j])\ny = matvec(identity, max, A, nothing)\n\n# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))\ny = matvec((a, b) -> exp(a - b), +, A, x)\n\n# In-place version\ndst = CUDA.zeros(Float32, 1000)\nmatvec!(dst, A, x)\n\nExtended Help\n\nFor tall matrices (many rows, few columns), each row is processed by a single block. For wide matrices (few rows, many columns), multiple blocks collaborate on each row via a number of blocks Nblocks computed from blocks_row. blocks_row is equal to Nblocks for a large row matrix.\n\nPre-allocating tmp avoids repeated allocation when calling matvec! in a loop. With FlagType=UInt8 (default), the flag buffer must be zeroed before each call. Using FlagType=UInt64 skips this zeroing by generating a random target flag at each call; correctness holds with probability 1 - n/2^64, which is negligible for practical n. Output element type is inferred as promote_op(g, promote_op(f, eltype(src), eltype(x))).\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.vecmat!","page":"API Reference","title":"KernelForge.vecmat!","text":"vecmat!(dst, x, A; kwargs...)\nvecmat!(f, op, dst, x, A; kwargs...)\n\nGPU parallel vector-matrix multiplication: dst = g(op(f(x .* A), dims=1)).\n\nFor standard matrix-vector product: vecmat!(dst, x, A) computes dst[j] = sum(x[i] * A[i,j]). When x = nothing, computes column reductions: dst[j] = sum(A[i,j]).\n\nArguments\n\nf=identity: Element-wise transformation applied to x[i] * A[i,j] (or A[i,j] if x=nothing)\nop=+: Reduction operator\ndst: Output vector of length p (number of columns)\nx: Input vector of length n (number of rows), or nothing for pure column reduction\nA: Input matrix of size (n, p)\n\nKeyword Arguments\n\ng=identity: Optional post-reduction transformation\ntmp=nothing: Pre-allocated temporary buffer (from get_allocation)\nNitem=nothing: Number of items per thread (auto-selected if nothing)\nNthreads=nothing: Number of threads per column reduction\nworkgroup=nothing: Workgroup size\nblocks=nothing: Maximum number of blocks\nFlagType=UInt8: Type for synchronization flags\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelForge.get_allocation","page":"API Reference","title":"KernelForge.get_allocation","text":"get_allocation(::Type{MapReduce1D}, src; blocks=DEFAULT_BLOCKS, out_eltype, FlagType=UInt8)\nget_allocation(::Type{MapReduce1D}, srcs::NTuple; blocks=DEFAULT_BLOCKS, out_eltype, FlagType=UInt8)\n\nAllocate temporary buffer for mapreduce1d!. Useful for repeated reductions.\n\nArguments\n\nsrc or srcs: Input GPU array(s) (used for backend)\n\nKeyword Arguments\n\nblocks=100: Number of blocks (must match the blocks used in mapreduce1d!)\nout_eltype: Element type for intermediate values. Pass promote_op(f, T, ...) for correct inference.\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ntmp = KernelForge.get_allocation(MapReduce1D, x; out_eltype=Float32)\ndst = CUDA.zeros(Float32, 1)\n\nfor i in 1:100\n    mapreduce1d!(identity, +, dst, x; tmp)\nend\n\n\n\n\n\nget_allocation(::Type{Scan1D}, dst, src; kwargs...)\n\nAllocate temporary buffer for scan!. Useful for repeated scans.\n\nArguments\n\ndst: Output GPU array (used for element type of intermediates)\nsrc: Input GPU array (used for backend)\n\nKeyword Arguments\n\nNitem=nothing: Items per thread (auto-selected if nothing)\nworkgroup=256: Workgroup size (must match the workgroup used in scan!)\nFlagType=UInt8: Synchronization flag type\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ndst = similar(x)\ntmp = KernelForge.get_allocation(Scan1D, dst, x)\n\nfor i in 1:100\n    scan!(+, dst, x; tmp)\nend\n\n\n\n\n\nget_allocation(::Type{Argmax1D}, src; blocks=100, out_eltype=nothing)\n\nAllocate temporary buffer for argmax1d!. Useful for repeated reductions.\n\nThe intermediate type is Tuple{out_eltype, Int} to track both value and index.\n\nArguments\n\nsrc or srcs: Input GPU array(s) (used for backend and default element type)\n\nKeyword Arguments\n\nblocks=100: Number of blocks (must match the blocks used in argmax1d!)\nout_eltype=nothing: Element type for intermediate values. If nothing, defaults to the element type of src. For proper type inference, pass promote_op(f, T, ...).\n\nExamples\n\nx = CUDA.rand(Float32, 10_000)\ntmp = KernelForge.get_allocation(Argmax1D, x)\ndst = CUDA.zeros(Int, 1)\n\nfor i in 1:100\n    argmax1d!(identity, >, dst, x; tmp)\nend\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides practical examples demonstrating KernelForge.jl's capabilities.","category":"section"},{"location":"examples/#Vectorized-Copy","page":"Examples","title":"Vectorized Copy","text":"Perform memory copies with vectorized loads and stores for improved bandwidth utilization:\n\nusing KernelForge\nusing CUDA\n\nsrc = CUDA.rand(Float32, 10^6)\ndst = similar(src)\n\n# Copy with vectorized loads and stores\nKernelForge.vcopy!(dst, src; Nitem=4)\n\n@assert dst ≈ src","category":"section"},{"location":"examples/#Custom-Types","page":"Examples","title":"Custom Types","text":"KernelForge supports copying custom struct types:\n\nusing KernelForge\nusing CUDA\n\nstruct Point3D\n    x::Float32\n    y::Float32\n    z::Float32\nend\n\nn = 100_000\nsrc = CuArray([Point3D(rand(), rand(), rand()) for _ in 1:n])\ndst = similar(src)\n\nKernelForge.vcopy!(dst, src; Nitem=2)\n\n@assert all(dst .== src)","category":"section"},{"location":"examples/#Set-Values","page":"Examples","title":"Set Values","text":"Fill an array with a constant value:\n\nusing KernelForge\nusing CUDA\n\ndst = CUDA.ones(UInt8, 100_000)\nKernelForge.setvalue!(dst, 0x00; Nitem=4)\n\n@assert all(dst .== 0x00)","category":"section"},{"location":"examples/#Map-Reduce","page":"Examples","title":"Map-Reduce","text":"","category":"section"},{"location":"examples/#Basic-Reductions","page":"Examples","title":"Basic Reductions","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 10^6)\n\n# Sum\ntotal = KernelForge.mapreduce(identity, +, x)\n\n# Sum of squares\nsum_sq = KernelForge.mapreduce(abs2, +, x)\n\n# Maximum\nmax_val = KernelForge.mapreduce(identity, max, x)\n\n# Minimum\nmin_val = KernelForge.mapreduce(identity, min, x)","category":"section"},{"location":"examples/#Post-Reduction-Transformation,-Example-with-KernelForge.UnitFloat8","page":"Examples","title":"Post-Reduction Transformation, Example with KernelForge.UnitFloat8","text":"Apply a function after reduction using the g parameter:\n\nusing KernelForge\nusing CUDA\n\nimport KernelForge: UnitFloat8\nx = CuArray{UnitFloat8}([rand(UnitFloat8) for _ in 1:10^6])\n\n# convert to float 32 for reduction to avoid overflow\nto_f32(x) = Float32(x)\n# then back to unitfloat8\nto_uif8(x) = UnitFloat8(x)\nsum_uif8 = KernelForge.mapreduce(to_f32, +, x; g=to_uif8)\n\n# compare the sign only :\n@assert sign(Float32(sum_uif8)) == sign(sum(Float32.(x)))\n\n# We can store only 8 bit, keep rather good precision (because of intermediate Float32 conversion) and get \n# same performance as for UInt8 addition !!!","category":"section"},{"location":"examples/#Dot-Product-and-Distance","page":"Examples","title":"Dot Product and Distance","text":"Reduce multiple arrays simultaneously:\n\nusing KernelForge\nusing CUDA\n\nn = 100_000\na = CUDA.rand(Float32, n)\nb = CUDA.rand(Float32, n)\n\n# Dot product: sum(a .* b)\ndot_prod = KernelForge.mapreduce1d((x, y) -> x * y, +, (a, b))\n\n# Euclidean distance: sqrt(sum((a - b)^2))\ndistance = KernelForge.mapreduce1d((x, y) -> (x - y)^2, +, (a, b); g=sqrt)\n\n# Weighted sum: sum(w .* x)\nw = CUDA.rand(Float32, n)\nx = CUDA.rand(Float32, n)\nweighted_sum = KernelForge.mapreduce1d((wi, xi) -> wi * xi, +, (w, x))","category":"section"},{"location":"examples/#2D-Reductions","page":"Examples","title":"2D Reductions","text":"Reduce along rows or columns:\n\nusing KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Column sums (reduce along dim=1)\ncol_sums = KernelForge.mapreduce(identity, +, A; dims=1)\n@assert size(col_sums) == (500,)\n\n# Row sums (reduce along dim=2)\nrow_sums = KernelForge.mapreduce(identity, +, A; dims=2)\n@assert size(row_sums) == (1000,)\n\n# Column means\nlet u = size(A, 1) # Note that the let block is necessary for compilation of the kernel\n    g(x)::Float32 = x / u\n    col_means = KernelForge.mapreduce(identity, +, A; dims=1, g=g)\nend\n# Row-wise sum of squares\nrow_ss = KernelForge.mapreduce(abs2, +, A; dims=2)\n\n# Row maximums\nrow_max = KernelForge.mapreduce(identity, max, A; dims=2)","category":"section"},{"location":"examples/#Higher-Dimensional-Reductions","page":"Examples","title":"Higher-Dimensional Reductions","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 20, 30, 40)\n\n# Reduce first dimension: (20, 30, 40) → (1, 30, 40)\nresult = KernelForge.mapreduce(identity, +, A; dims=1)\n@assert size(result) == (1, 30, 40)\n\n# Reduce first two dimensions: (20, 30, 40) → (1, 1, 40)\nresult = KernelForge.mapreduce(identity, +, A; dims=(1, 2))\n@assert size(result) == (1, 1, 40)\n\n# Reduce last dimension: (20, 30, 40) → (20, 30, 1)\nresult = KernelForge.mapreduce(identity, +, A; dims=3)\n@assert size(result) == (20, 30, 1)\n\n# Reduce last two dimensions: (20, 30, 40) → (20, 1, 1)\nresult = KernelForge.mapreduce(identity, +, A; dims=(2, 3))\n@assert size(result) == (20, 1, 1)\n\n# General reduction on non-contiguous dims: (3, 4, 5, 6, 7) → (3, 1, 5, 1, 1)\nB = CUDA.rand(Float32, 3, 4, 5, 6, 7)\nresult = KernelForge.mapreduce(identity, +, B; dims=(2, 4, 5))\n@assert size(result) == (3, 1, 5, 1, 1)","category":"section"},{"location":"examples/#Custom-Structs","page":"Examples","title":"Custom Structs","text":"using KernelForge\nusing CUDA\n\nstruct Stats\n    sum::Float32\n    sum_sq::Float32\n    count::Float32\nend\n\n# Map: convert each value to Stats\nf(x) = Stats(x, x^2, 1f0)\n\n# Reduce: combine Stats\nop(a::Stats, b::Stats) = Stats(a.sum + b.sum, a.sum_sq + b.sum_sq, a.count + b.count)\n\nx = CUDA.rand(Float32, 100_000)\nresult = KernelForge.mapreduce(f, op, x)\n\nmean = result.sum / result.count\nvariance = result.sum_sq / result.count - mean^2","category":"section"},{"location":"examples/#Operations-on-Views","page":"Examples","title":"Operations on Views","text":"All KernelForge functions work transparently on GPU array views, with no copies:\n\nusing KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 100, 200)\n\n# Reduce over a column slice\ncol_slice = view(A, :, 50:150)\nresult = KernelForge.mapreduce(identity, +, col_slice; dims=1)\n@assert size(result) == (1, 101)\n\n# Full reduction on a row slice\nrow_slice = view(A, 10:90, :)\ntotal = KernelForge.mapreduce(identity, +, row_slice)\n@assert total ≈ sum(Array(A)[10:90, :])","category":"section"},{"location":"examples/#Prefix-Scan","page":"Examples","title":"Prefix Scan","text":"","category":"section"},{"location":"examples/#Basic-Scans","page":"Examples","title":"Basic Scans","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 10^6)\ndst = similar(x)\n\n# Cumulative sum\nKernelForge.scan!(+, dst, x)\n@assert dst ≈ CuArray(accumulate(+, Array(x)))\n\n# Cumulative product\nKernelForge.scan!(*, dst, x)\n@assert dst ≈ CuArray(accumulate(*, Array(x)))\n\n# Cumulative maximum\nKernelForge.scan!(max, dst, x)\n@assert dst ≈ CuArray(accumulate(max, Array(x)))","category":"section"},{"location":"examples/#Scan-with-Map-Function","page":"Examples","title":"Scan with Map Function","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 10_000)\n\n# Cumulative sum of squares\nresult = KernelForge.scan(abs2, +, x)\n@assert result ≈ CuArray(accumulate(+, Array(x).^2))\n\n# Cumulative sum of absolute values\nresult = KernelForge.scan(abs, +, x)","category":"section"},{"location":"examples/#Scan-on-Views","page":"Examples","title":"Scan on Views","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 1_000)\ndst = CUDA.zeros(Float32, 1_000)\n\n# Scan over a subrange, writing into a subrange of dst\nKernelForge.scan!(+, view(dst, 3:800), view(x, 3:800))\n\n@assert Array(dst)[3:800] ≈ accumulate(+, Array(x)[3:800])\n# Elements outside the view are untouched\n@assert all(Array(dst)[1:2] .== 0f0)","category":"section"},{"location":"examples/#Non-Commutative-Operations:-Quaternions","page":"Examples","title":"Non-Commutative Operations: Quaternions","text":"KernelForge scan function (not mapreduce) correctly handles non-commutative operations:\n\nusing KernelForge\nusing CUDA\nusing Quaternions\n\nn = 100_000\n\n# Generate random unit quaternions\nsrc_cpu = [QuaternionF64((x ./ sqrt(sum(x.^2)))...) for x in eachcol(randn(4, n))]\nsrc = CuArray(src_cpu)\ndst = similar(src)\n\n# Quaternion multiplication is non-commutative: q1 * q2 ≠ q2 * q1\nKernelForge.scan!(*, dst, src)\n\n@assert dst ≈ CuArray(accumulate(*, src_cpu))","category":"section"},{"location":"examples/#Matrix-Vector-Operations","page":"Examples","title":"Matrix-Vector Operations","text":"","category":"section"},{"location":"examples/#Standard-Matrix-Vector-Multiply","page":"Examples","title":"Standard Matrix-Vector Multiply","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# y = A * x\ny = KernelForge.matvec(A, x)\n@assert y ≈ A * x\n\n# In-place version\ndst = CUDA.zeros(Float32, 1000)\nKernelForge.matvec!(dst, A, x)\n@assert dst ≈ A * x","category":"section"},{"location":"examples/#Row-wise-Reductions","page":"Examples","title":"Row-wise Reductions","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Row sums: y[i] = sum(A[i, :])\nrow_sums = KernelForge.matvec(A, nothing)\n@assert row_sums ≈ vec(sum(A; dims=2))\n\n# Row maximums: y[i] = max_j(A[i, j])\nrow_max = KernelForge.matvec(identity, max, A, nothing)\n@assert row_max ≈ vec(maximum(A; dims=2))\n\n# Row minimums\nrow_min = KernelForge.matvec(identity, min, A, nothing)\n@assert row_min ≈ vec(minimum(A; dims=2))","category":"section"},{"location":"examples/#Custom-Operations","page":"Examples","title":"Custom Operations","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\n\n# Softmax numerator: y[i] = sum_j(exp(A[i,j] - x[j]))\ny = KernelForge.matvec((a, b) -> exp(a - b), +, A, x)\n\n# Row-wise dot product with scaling: y[i] = sqrt(sum_j(A[i,j]^2 * x[j]^2))\ny = KernelForge.matvec((a, b) -> a^2 * b^2, +, A, x; g=sqrt)","category":"section"},{"location":"examples/#Custom-Struct-Output","page":"Examples","title":"Custom Struct Output","text":"using KernelForge\nusing CUDA\n\nstruct Vec3\n    x::Float32\n    y::Float32\n    z::Float32\nend\n\n# Map: combine matrix and vector elements\nf(a, b) = Vec3(a * b, a + b, a - b)\n\n# Reduce: component-wise sum\nop(v1::Vec3, v2::Vec3) = Vec3(v1.x + v2.x, v1.y + v2.y, v1.z + v2.z)\n\nA = CUDA.rand(Float32, 200, 500)\nx = CUDA.rand(Float32, 500)\ndst = CuArray{Vec3}(undef, 200)\n\nKernelForge.matvec!(f, op, dst, A, x)","category":"section"},{"location":"examples/#Matvec-on-Views","page":"Examples","title":"Matvec on Views","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 100, 200)\nx = CUDA.rand(Float32, 200)\n\n# Multiply using only a submatrix and a subvector\nresult = KernelForge.matvec(view(A, 10:90, 50:150), view(x, 50:150))\n@assert Array(result) ≈ Array(A)[10:90, 50:150] * Array(x)[50:150]","category":"section"},{"location":"examples/#Vector-Matrix-Operations","page":"Examples","title":"Vector-Matrix Operations","text":"","category":"section"},{"location":"examples/#Standard-Vector-Matrix-Multiply","page":"Examples","title":"Standard Vector-Matrix Multiply","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 1000)\n\n# y = x' * A (column-wise weighted sum)\ndst = CUDA.zeros(Float32, 500)\nKernelForge.vecmat!(dst, x, A)\n@assert dst ≈ vec(x' * A)","category":"section"},{"location":"examples/#Column-wise-Reductions","page":"Examples","title":"Column-wise Reductions","text":"using KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 1000, 500)\n\n# Column sums: y[j] = sum(A[:, j])\ndst = CUDA.zeros(Float32, 500)\nKernelForge.vecmat!(dst, nothing, A)\n@assert dst ≈ vec(sum(A; dims=1))","category":"section"},{"location":"examples/#Search","page":"Examples","title":"Search","text":"","category":"section"},{"location":"examples/#findfirst","page":"Examples","title":"findfirst","text":"Find the index of the first element satisfying a predicate:\n\nusing KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 100_000)\n\n# First element above threshold\ni = KernelForge.findfirst(>(0.999f0), x)\n# i is either an Int index or nothing\n\n# First exact match\nCUDA.@allowscalar x[42] = -1f0\ni = KernelForge.findfirst(==(-1f0), x)\n@assert i == 42","category":"section"},{"location":"examples/#findlast","page":"Examples","title":"findlast","text":"using KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 100_000)\nCUDA.@allowscalar x[99_000] = -1f0\ni = KernelForge.findlast(==(-1f0), x)\n@assert i == 99_000","category":"section"},{"location":"examples/#findfirst-on-a-2D-array","page":"Examples","title":"findfirst on a 2D array","text":"For multidimensional arrays, a CartesianIndex is returned:\n\nusing KernelForge\nusing CUDA\n\nA = CUDA.rand(Float32, 100, 200)\nCUDA.@allowscalar A[37, 82] = -1f0\n\nidx = KernelForge.findfirst(==(-1f0), A)\n@assert idx == CartesianIndex(37, 82)","category":"section"},{"location":"examples/#findfirst-on-custom-structs","page":"Examples","title":"findfirst on custom structs","text":"using KernelForge\nusing CUDA\n\nstruct Particle\n    mass::Float32\n    charge::Float32\nend\n\nparticles = CuArray([Particle(rand(), rand() - 0.5f0) for _ in 1:10_000])\n\n# First negatively charged particle\ni = KernelForge.findfirst(p -> p.charge < 0f0, particles)","category":"section"},{"location":"examples/#argmax-/-argmin","page":"Examples","title":"argmax / argmin","text":"","category":"section"},{"location":"examples/#Basic-usage","page":"Examples","title":"Basic usage","text":"argmax and argmin return the index of the extremal element:\n\nusing KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 100_000)\n\ni = KernelForge.argmax(x)   # index of maximum\nj = KernelForge.argmin(x)   # index of minimum\n\n@assert Array(x)[i] == maximum(Array(x))\n@assert Array(x)[j] == minimum(Array(x))","category":"section"},{"location":"examples/#Custom-relation","page":"Examples","title":"Custom relation","text":"Pass any binary relation to find the extremum under a custom ordering:\n\nusing KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 100_000)\n\n# Equivalent to argmax\ni = KernelForge.argmax(>, x)\n\n# Equivalent to argmin\nj = KernelForge.argmax(<, x)","category":"section"},{"location":"examples/#argmax-with-custom-structs","page":"Examples","title":"argmax with custom structs","text":"Use argmax1d directly with a map function f and a relation rel for full control:\n\nusing KernelForge\nusing CUDA\n\nstruct PriorityItem\n    priority::Int32\n    cost::Float32\nend\n\n# \"better\" means higher priority; ties broken by lower cost\nbetter(a::PriorityItem, b::PriorityItem) =\n    a.priority > b.priority || (a.priority == b.priority && a.cost < b.cost)\n\nitems = CuArray([PriorityItem(rand(Int32(1):Int32(5)), rand(Float32)) for _ in 1:10_000])\n\n# Index of the best item\ni = KernelForge.argmax(better, items)","category":"section"},{"location":"examples/#Tie-breaking","page":"Examples","title":"Tie-breaking","text":"When multiple elements share the extremal value, the first index is always returned:\n\nusing KernelForge\nusing CUDA\n\nx = CuArray(Float32[5, 5, 5, 5, 5])\n@assert KernelForge.argmax(x) == 1\n@assert KernelForge.argmin(x) == 1","category":"section"},{"location":"examples/#Pre-allocating-Temporary-Buffers","page":"Examples","title":"Pre-allocating Temporary Buffers","text":"For repeated operations, pre-allocate temporary buffers to avoid allocation overhead:\n\nusing KernelForge\nusing CUDA\n\nx = CUDA.rand(Float32, 100_000)\ndst = similar(x)\n\n# Pre-allocate for scan\ntmp = KernelForge.get_allocation(KernelForge.scan!, dst, x)\n\n# Reuse in a loop\nfor i in 1:100\n    CUDA.rand!(x)  # new random data\n    KernelForge.scan!(+, dst, x; tmp=tmp)\nend","category":"section"},{"location":"examples/#Complete-Example:-Online-Statistics","page":"Examples","title":"Complete Example: Online Statistics","text":"Compute running mean and variance in a single pass:\n\nusing KernelForge\nusing CUDA\n\nstruct RunningStats\n    n::Float32\n    mean::Float32\n    m2::Float32  # sum of squared deviations\nend\n\n# Welford's online algorithm for combining statistics\nfunction combine(a::RunningStats, b::RunningStats)\n    n = a.n + b.n\n    delta = b.mean - a.mean\n    mean = a.mean + delta * b.n / n\n    m2 = a.m2 + b.m2 + delta^2 * a.n * b.n / n\n    return RunningStats(n, mean, m2)\nend\n\n# Initialize each element as a single observation\ninit(x) = RunningStats(1f0, x, 0f0)\n\nx = CUDA.rand(Float32, 1_000_000)\ndst = CuArray{RunningStats}(undef, length(x))\n\nKernelForge.scan!(init, combine, dst, x)\n\n# Final statistics\nfinal_stats = Array(dst)[end]\nmean = final_stats.mean\nvariance = final_stats.m2 / final_stats.n\nstd = sqrt(variance)","category":"section"},{"location":"#KernelForge.jl","page":"Home","title":"KernelForge.jl","text":"High-performance, portable GPU primitives for Julia. A pure Julia implementation delivering performance competitive with optimized CUDA C++ libraries.\n\nwarning: Experimental Status\nThis package is in an experimental phase. Although extensive testing has been performed, no bounds checking is performed, which may lead to unexpected behavior with out-of-bounds access. Correctness and performance have been validated only on a small NVIDIA RTX 1000.\n\ninfo: Architecture & Contributions\nKernelForge.jl builds on KernelAbstractions.jl for GPU kernel dispatch. However, certain low-level operations—including warp shuffle instructions, vectorized memory access, and memory ordering semantics—are not yet available in KA.jl, so we use KernelIntrinsics.jl for these primitives. As KernelIntrinsics.jl currently supports only CUDA, KernelForge.jl is likewise restricted to CUDA. The core contribution of this package lies in the GPU kernel implementations themselves, designed to be portable once the underlying intrinsics become available on other backends. Extending support to AMD and Intel GPUs would primarily require work in KernelIntrinsics.jl, with minimal adaptations in KernelForge.jl.\n\nnote: Citation\nA paper describing this work is in preparation. If you use this code, please check back for citation details.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"KernelForge\")","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Map-reduce with custom functions and operators, supporting arbitrary dimensions and multidimensional arrays including non-contiguous dimension reduction via mapreduce_dims\nPrefix scan supporting non-commutative operations\nMatrix-vector operations with customizable element-wise and reduction operations\nSearch — findfirst, findlast, argmax, argmin on GPU arrays\nVectorized copy with configurable load/store widths\nViews and strided arrays supported throughout, enabled by KernelIntrinsics.jl's vectorized memory access primitives which correctly handle non-contiguous memory layouts\nCurrently CUDA-only; cross-platform support via KernelAbstractions.jl planned\nIncludes UnitFloat8, a custom 8-bit floating-point type with range (-1, 1) for testing","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using KernelForge\nusing CUDA\n\n# Prefix scan\nsrc = CUDA.rand(Float32, 10^6)\ndst = similar(src)\nKernelForge.scan!(+, dst, src)\n\n# Matrix-vector multiply\nA = CUDA.rand(Float32, 1000, 500)\nx = CUDA.rand(Float32, 500)\ny = KernelForge.matvec(A, x)\n\n# Map-reduce (full reduction)\ntotal = KernelForge.mapreduce(abs2, +, src)\n\n# Map-reduce over specific dimensions\nB = CUDA.rand(Float32, 4, 8, 16)\nresult = KernelForge.mapreduce(identity, +, B; dims=(1, 3))  # shape: (1, 8, 1)\n\n# Views are supported\nv = view(src, 1:2:10^6)\ntotal_view = KernelForge.mapreduce(abs2, +, v)\n\n# Search\ni = KernelForge.findfirst(>(0.99f0), src)\nj = KernelForge.argmax(src)","category":"section"},{"location":"#Acknowledgments","page":"Home","title":"Acknowledgments","text":"This package builds on the foundation provided by KernelAbstractions.jl and CUDA.jl. The API design draws inspiration from several packages in the Julia ecosystem. Development of the API and documentation was assisted by Claude (Anthropic).","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT","category":"section"}]
}
